{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Download yolov3-tiny.weights, yolov3-tiny.cfg file</h1>\n",
    "\n",
    "[download link](https://github.com/smarthomefans/darknet-test/blob/master/yolov3-tiny.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Edit yolov3-tiny.cfg file</h1>\n",
    "\n",
    "* make [net] as comment\n",
    "<p> # [net]\n",
    "<p> # # Testing\n",
    "<p> # batch=1\n",
    "<p> # subdivisions=1\n",
    "<p> # # Training\n",
    "<p> # # batch=64\n",
    "<p> # # subdivisions=2\n",
    "<p> # width=416\n",
    "<p> # height=416\n",
    "<p> # channels=3\n",
    "<p> # momentum=0.9\n",
    "<p> # decay=0.0005\n",
    "<p> # angle=0\n",
    "<p> # saturation = 1.5\n",
    "<p> # exposure = 1.5\n",
    "<p> # hue=.1\n",
    "<p> \n",
    "<p> # learning_rate=0.001\n",
    "<p> # burn_in=1000\n",
    "<p> # max_batches = 500200\n",
    "<p> # policy=steps\n",
    "<p> # steps=400000,450000\n",
    "<p> # scales=.1,.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read file in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to JSON with connections: yolov3-tiny_weights_with_connections.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. 가중치 파일 읽기\n",
    "def load_weights(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        header = np.fromfile(f, dtype=np.int32, count=5)  # 헤더 읽기 (5개 정수)\n",
    "        weights = np.fromfile(f, dtype=np.float32)        # 나머지 가중치 읽기\n",
    "    return header, weights\n",
    "\n",
    "# 2. YOLO 구성 파일 파싱\n",
    "def parse_cfg(cfg_file):\n",
    "    layers = []\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        lines = [x.strip() for x in lines if x and not x.startswith(\"#\")]  # 주석 제거\n",
    "        layer = {}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"[\"):  # 새 레이어 정의 시작\n",
    "                if layer:\n",
    "                    layers.append(layer)  # 이전 레이어 저장\n",
    "                layer = {\"type\": line[1:-1]}  # 레이어 타입\n",
    "            else:\n",
    "                key, value = line.split(\"=\")\n",
    "                layer[key.strip()] = value.strip()\n",
    "        layers.append(layer)  # 마지막 레이어 추가\n",
    "    return layers\n",
    "\n",
    "# 3. 레이어별로 가중치를 매핑\n",
    "def load_weights_to_layers(layers, weights):\n",
    "    ptr = 0\n",
    "    layer_weights = []\n",
    "    previous_output_channels = 3  # 초기 입력 채널 (RGB 이미지)\n",
    "\n",
    "    for layer_idx, layer in enumerate(layers):\n",
    "        if layer[\"type\"] == \"convolutional\":  # Conv 레이어 처리\n",
    "            filters = int(layer[\"filters\"])\n",
    "            size = int(layer[\"size\"])\n",
    "            stride = int(layer.get(\"stride\", 1))\n",
    "            pad = int(layer.get(\"pad\", 0))\n",
    "            activation = layer.get(\"activation\", \"linear\")\n",
    "\n",
    "            # 입력 채널 동적 계산\n",
    "            input_channels = previous_output_channels\n",
    "\n",
    "            # Conv2D 가중치 추출\n",
    "            weight_size = filters * input_channels * size * size\n",
    "            conv_weights = weights[ptr:ptr + weight_size].reshape((filters, input_channels, size, size))\n",
    "            ptr += weight_size\n",
    "\n",
    "            # 커널 연결 관계 주석 생성\n",
    "            kernel_connections = [\n",
    "                f\"Output channel {f_idx} is connected to input channel {c_idx}\"\n",
    "                for f_idx in range(filters)\n",
    "                for c_idx in range(input_channels)\n",
    "            ]\n",
    "\n",
    "            if \"batch_normalize\" in layer:\n",
    "                # BatchNorm 파라미터 (scale, bias, mean, variance)\n",
    "                bn_weights = weights[ptr:ptr + 4 * filters].reshape((4, filters))\n",
    "                ptr += 4 * filters\n",
    "                layer_weights.append({\n",
    "                    \"type\": \"convolutional\",\n",
    "                    \"layer_index\": layer_idx,\n",
    "                    \"filters\": filters,\n",
    "                    \"input_channels\": input_channels,\n",
    "                    \"kernel_size\": size,\n",
    "                    \"stride\": stride,\n",
    "                    \"padding\": pad,\n",
    "                    \"activation\": activation,\n",
    "                    \"conv_weights\": conv_weights.tolist(),\n",
    "                    \"batch_normalize\": True,\n",
    "                    \"bn_weights\": {\n",
    "                        \"scale\": bn_weights[0].tolist(),\n",
    "                        \"bias\": bn_weights[1].tolist(),\n",
    "                        \"mean\": bn_weights[2].tolist(),\n",
    "                        \"variance\": bn_weights[3].tolist()\n",
    "                    },\n",
    "                    \"kernel_connections\": kernel_connections  # 연결 관계 주석\n",
    "                })\n",
    "            else:\n",
    "                # Bias 추출\n",
    "                bias_weights = weights[ptr:ptr + filters]\n",
    "                ptr += filters\n",
    "                layer_weights.append({\n",
    "                    \"type\": \"convolutional\",\n",
    "                    \"layer_index\": layer_idx,\n",
    "                    \"filters\": filters,\n",
    "                    \"input_channels\": input_channels,\n",
    "                    \"kernel_size\": size,\n",
    "                    \"stride\": stride,\n",
    "                    \"padding\": pad,\n",
    "                    \"activation\": activation,\n",
    "                    \"conv_weights\": conv_weights.tolist(),\n",
    "                    \"batch_normalize\": False,\n",
    "                    \"bias_weights\": bias_weights.tolist(),\n",
    "                    \"kernel_connections\": kernel_connections  # 연결 관계 주석\n",
    "                })\n",
    "\n",
    "            # 다음 레이어의 입력 채널 업데이트\n",
    "            previous_output_channels = filters\n",
    "\n",
    "        else:\n",
    "            # Conv 레이어가 아닌 경우\n",
    "            layer_weights.append({\"type\": layer[\"type\"], \"comment\": \"No weights for this layer.\"})\n",
    "\n",
    "    return layer_weights\n",
    "\n",
    "# 4. JSON으로 저장\n",
    "def save_weights_to_json(layer_weights, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump({\"layers\": layer_weights}, f, indent=4)\n",
    "\n",
    "# 5. 경로 설정 및 실행\n",
    "cfg_file = \"yolov3-tiny.cfg\"             # YOLOv3-tiny 구성 파일 경로\n",
    "weights_file = \"yolov3-tiny.weights\"     # YOLOv3-tiny 가중치 파일 경로\n",
    "json_output_file = \"yolov3-tiny_weights_with_connections.json\"  # JSON 출력 파일\n",
    "\n",
    "# 실행\n",
    "header, weights = load_weights(weights_file)  # 가중치 읽기\n",
    "layers = parse_cfg(cfg_file)                  # 구성 파일 파싱\n",
    "layer_weights = load_weights_to_layers(layers, weights)  # 가중치 매핑\n",
    "\n",
    "# JSON 저장\n",
    "save_weights_to_json(layer_weights, json_output_file)\n",
    "\n",
    "print(f\"Weights saved to JSON with connections: {json_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read file in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to NumPy files: yolov3-tiny_numpy_weights\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. 가중치 파일 읽기\n",
    "def load_weights(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        header = np.fromfile(f, dtype=np.int32, count=5)  # 헤더 읽기 (5개 정수)\n",
    "        weights = np.fromfile(f, dtype=np.float32)        # 나머지 가중치 읽기\n",
    "    return header, weights\n",
    "\n",
    "# 2. YOLO 구성 파일 파싱\n",
    "def parse_cfg(cfg_file):\n",
    "    layers = []\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        lines = [x.strip() for x in lines if x and not x.startswith(\"#\")]  # 주석 제거\n",
    "        layer = {}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"[\"):  # 새 레이어 정의 시작\n",
    "                if layer:\n",
    "                    layers.append(layer)  # 이전 레이어 저장\n",
    "                layer = {\"type\": line[1:-1]}  # 레이어 타입\n",
    "            else:\n",
    "                key, value = line.split(\"=\")\n",
    "                layer[key.strip()] = value.strip()\n",
    "        layers.append(layer)  # 마지막 레이어 추가\n",
    "    return layers\n",
    "\n",
    "# 3. 레이어별로 가중치를 매핑\n",
    "def load_weights_to_layers(layers, weights):\n",
    "    ptr = 0\n",
    "    layer_weights = []\n",
    "    previous_output_channels = 3  # 초기 입력 채널 (RGB 이미지)\n",
    "\n",
    "    for layer in layers:\n",
    "        if layer[\"type\"] == \"convolutional\":  # Conv 레이어 처리\n",
    "            filters = int(layer[\"filters\"])\n",
    "            size = int(layer[\"size\"])\n",
    "            stride = int(layer.get(\"stride\", 1))\n",
    "            pad = int(layer.get(\"pad\", 0))\n",
    "            activation = layer.get(\"activation\", \"linear\")\n",
    "\n",
    "            # 입력 채널 동적 계산\n",
    "            input_channels = previous_output_channels\n",
    "\n",
    "            # Conv2D 가중치 추출\n",
    "            weight_size = filters * input_channels * size * size\n",
    "            conv_weights = weights[ptr:ptr + weight_size].reshape((filters, input_channels, size, size))\n",
    "            ptr += weight_size\n",
    "\n",
    "            if \"batch_normalize\" in layer:\n",
    "                # BatchNorm 파라미터 (scale, bias, mean, variance)\n",
    "                bn_weights = weights[ptr:ptr + 4 * filters].reshape((4, filters))\n",
    "                ptr += 4 * filters\n",
    "                layer_weights.append({\n",
    "                    \"conv_weights\": conv_weights,\n",
    "                    \"bn_weights\": {\n",
    "                        \"scale\": bn_weights[0],\n",
    "                        \"bias\": bn_weights[1],\n",
    "                        \"mean\": bn_weights[2],\n",
    "                        \"variance\": bn_weights[3]\n",
    "                    },\n",
    "                    \"activation\": activation,\n",
    "                    \"stride\": stride,\n",
    "                    \"padding\": pad\n",
    "                })\n",
    "            else:\n",
    "                # Bias 추출\n",
    "                bias_weights = weights[ptr:ptr + filters]\n",
    "                ptr += filters\n",
    "                layer_weights.append({\n",
    "                    \"conv_weights\": conv_weights,\n",
    "                    \"bias_weights\": bias_weights,\n",
    "                    \"activation\": activation,\n",
    "                    \"stride\": stride,\n",
    "                    \"padding\": pad\n",
    "                })\n",
    "\n",
    "            # 다음 레이어의 입력 채널 업데이트\n",
    "            previous_output_channels = filters\n",
    "\n",
    "        else:\n",
    "            # Conv 레이어가 아닌 경우\n",
    "            layer_weights.append(None)\n",
    "\n",
    "    return layer_weights\n",
    "\n",
    "# 4. 가중치를 NumPy 파일로 저장\n",
    "def save_weights_to_numpy(layer_weights, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i, weights in enumerate(layer_weights):\n",
    "        if weights is not None:\n",
    "            np.save(os.path.join(output_dir, f\"layer_{i}_conv.npy\"), weights[\"conv_weights\"])\n",
    "            if \"bn_weights\" in weights:\n",
    "                for key, value in weights[\"bn_weights\"].items():\n",
    "                    np.save(os.path.join(output_dir, f\"layer_{i}_bn_{key}.npy\"), value)\n",
    "            if \"bias_weights\" in weights:\n",
    "                np.save(os.path.join(output_dir, f\"layer_{i}_bias.npy\"), weights[\"bias_weights\"])\n",
    "\n",
    "# 5. 경로 설정 및 실행\n",
    "cfg_file = \"yolov3-tiny.cfg\"             # YOLOv3-tiny 구성 파일 경로\n",
    "weights_file = \"yolov3-tiny.weights\"     # YOLOv3-tiny 가중치 파일 경로\n",
    "numpy_output_dir = \"yolov3-tiny_numpy_weights\"  # NumPy 출력 디렉토리\n",
    "\n",
    "# 실행\n",
    "header, weights = load_weights(weights_file)  # 가중치 읽기\n",
    "layers = parse_cfg(cfg_file)                  # 구성 파일 파싱\n",
    "layer_weights = load_weights_to_layers(layers, weights)  # 가중치 매핑\n",
    "\n",
    "# 결과 저장 (NumPy)\n",
    "save_weights_to_numpy(layer_weights, numpy_output_dir)\n",
    "\n",
    "print(f\"Weights saved to NumPy files: {numpy_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Extract Numpy to txt(16b fixed point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Weights Shape: (16, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "layer = 0  # 처리할 레이어 인덱스\n",
    "\n",
    "# 고정소수점 변환에 사용될 스케일링 값\n",
    "FIXED_POINT_SCALE = 2**8  # 고정소수점 스케일링 (소수점 이하 8비트)\n",
    "\n",
    "def to_fixed_point(value, scale):\n",
    "    \"\"\"\n",
    "    실수를 고정소수점 16진수로 변환\n",
    "    - value: 변환할 실수 값 배열\n",
    "    - scale: 고정소수점 스케일링 값 (예: 2^8)\n",
    "    \"\"\"\n",
    "    scaled = np.round(value * scale).astype(int)  # 스케일링 후 정수 변환\n",
    "    hex_value = np.where(\n",
    "        scaled >= 0,\n",
    "        scaled,\n",
    "        (1 << 16) + scaled,  # 음수는 16비트 2의 보수 처리\n",
    "    )\n",
    "    return [f\"{x:04X}\" for x in hex_value]  # 4자리 16진수 문자열 반환\n",
    "\n",
    "def generate_kernel_connections(out_channels, in_channels):\n",
    "    \"\"\"\n",
    "    각 출력 채널과 입력 채널 간의 연결 정보를 생성\n",
    "    - out_channels: 출력 채널 개수\n",
    "    - in_channels: 입력 채널 개수\n",
    "    \"\"\"\n",
    "    connections = []\n",
    "    for out_ch in range(out_channels):\n",
    "        for in_ch in range(in_channels):\n",
    "            connections.append(f\"Output channel {out_ch} is connected to input channel {in_ch}\")\n",
    "    return connections\n",
    "\n",
    "\n",
    "# 1. Conv 레이어 가중치 및 편향 로드\n",
    "try:\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_conv.npy\"):\n",
    "        conv_weights = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_conv.npy\")\n",
    "        print(\"Conv Weights Shape:\", conv_weights.shape)\n",
    "\n",
    "        out_channels, in_channels, kernel_h, kernel_w = conv_weights.shape\n",
    "        kernel_size = kernel_h * kernel_w  # 한 커널당 요소 개수\n",
    "        kernel_connections = generate_kernel_connections(out_channels, in_channels)\n",
    "\n",
    "        conv_weights_flat = conv_weights.flatten()\n",
    "        conv_weights_hex = to_fixed_point(conv_weights_flat, FIXED_POINT_SCALE)\n",
    "\n",
    "        with open(f\"layer_{layer}_conv_weights_fixed.txt\", \"w\") as f:\n",
    "            kernel_index = 0\n",
    "            for i in range(0, len(conv_weights_flat), kernel_size):\n",
    "                kernel_values = conv_weights_flat[i:i + kernel_size]\n",
    "                kernel_hex = conv_weights_hex[i:i + kernel_size]\n",
    "\n",
    "                for original, hex_value in zip(kernel_values, kernel_hex):\n",
    "                    f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "\n",
    "                connection_comment = kernel_connections[kernel_index]\n",
    "                f.write(f\"// {connection_comment}\\n\\n\")\n",
    "                kernel_index += 1\n",
    "\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bias.npy\"):\n",
    "        conv_bias = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_bias.npy\")\n",
    "        conv_bias_hex = to_fixed_point(conv_bias, FIXED_POINT_SCALE)\n",
    "        with open(f\"layer_{layer}_conv_bias_fixed.txt\", \"w\") as f:\n",
    "            for original, hex_value in zip(conv_bias, conv_bias_hex):\n",
    "                f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Conv weights or biases: {e}\")\n",
    "\n",
    "\n",
    "# 2. BatchNorm 파라미터 로드\n",
    "try:\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_scale.npy\"):\n",
    "        scale = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_scale.npy\")\n",
    "        scale_hex = to_fixed_point(scale, FIXED_POINT_SCALE)\n",
    "        with open(f\"layer_{layer}_bn_scale_fixed.txt\", \"w\") as f:\n",
    "            for original, hex_value in zip(scale, scale_hex):\n",
    "                f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_bias.npy\"):\n",
    "        bias = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_bias.npy\")\n",
    "        bias_hex = to_fixed_point(bias, FIXED_POINT_SCALE)\n",
    "        with open(f\"layer_{layer}_bn_bias_fixed.txt\", \"w\") as f:\n",
    "            for original, hex_value in zip(bias, bias_hex):\n",
    "                f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_mean.npy\"):\n",
    "        mean = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_mean.npy\")\n",
    "        mean_hex = to_fixed_point(mean, FIXED_POINT_SCALE)\n",
    "        with open(f\"layer_{layer}_bn_mean_fixed.txt\", \"w\") as f:\n",
    "            for original, hex_value in zip(mean, mean_hex):\n",
    "                f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "\n",
    "    if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_variance.npy\"):\n",
    "        variance = np.load(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_variance.npy\")\n",
    "        variance_hex = to_fixed_point(variance, FIXED_POINT_SCALE)\n",
    "        with open(f\"layer_{layer}_bn_variance_fixed.txt\", \"w\") as f:\n",
    "            for original, hex_value in zip(variance, variance_hex):\n",
    "                f.write(f\"{hex_value} // {original:.6f}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing BatchNorm parameters: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Make layer # to C header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before, should run numpy to txt code\n",
    "\n",
    "layer = 0\n",
    "# conv 생성 부분 따로 편집하기\n",
    "try: \n",
    "    with open(f\"../C/header/layer_{layer}_conv_param.h\", \"w\") as f:\n",
    "        if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_conv.npy\"):\n",
    "            f.write(f\"// Conv layer {layer} weights\\n\")\n",
    "            f.write(f\"s16 layer{layer}_conv_weight[][] = {{{{\\n\")\n",
    "            for i in range(0, len(conv_weights_flat), kernel_size):\n",
    "                temp = 0\n",
    "                kernel_values = conv_weights_flat[i:i + kernel_size]\n",
    "                kernel_hex = conv_weights_hex[i:i + kernel_size]\n",
    "                for hex_value in kernel_hex:\n",
    "                    temp += 1 \n",
    "                    if(temp == len(kernel_hex)):\n",
    "                        f.write(f\"{hex_value}\\n\")\n",
    "                    else: \n",
    "                        f.write(f\"{hex_value},\\n\")\n",
    "\n",
    "                if i + kernel_size >= len(conv_weights_flat):  \n",
    "                    f.write(\"}\\n\")           \n",
    "                else: \n",
    "                    f.write(\"},\\n\") \n",
    "            f.write(\"};\\n\") \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Conv weights or biases: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(f\"../C/header/layer_{layer}_bn_param.h\", \"w\") as f:\n",
    "        if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_scale.npy\"):\n",
    "            f.write(\"// bn scale\\n\")\n",
    "            f.write(f\"s16 layer{layer}_bn_scale[] = {{\\n\")\n",
    "            for hex_val in scale_hex:  \n",
    "                f.write(f\"{hex_val},\\n\")  \n",
    "            f.write(\"};\\n\")  \n",
    "\n",
    "        if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_bias.npy\"):\n",
    "            f.write(\"// bn bias\\n\")\n",
    "            f.write(f\"s16 layer{layer}_bn_bias[] = {{\\n\")\n",
    "            for hex_val in bias_hex:  \n",
    "                f.write(f\"{hex_val},\\n\")  \n",
    "            f.write(\"};\\n\") \n",
    "\n",
    "        if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_mean.npy\"):\n",
    "            f.write(\"// bn mean\\n\")\n",
    "            f.write(f\"s16 layer{layer}_bn_mean[] = {{\\n\")\n",
    "            for hex_val in mean_hex:  \n",
    "                f.write(f\"{hex_val},\\n\")  \n",
    "            f.write(\"};\\n\") \n",
    "\n",
    "        if os.path.exists(f\"yolov3-tiny_numpy_weights/layer_{layer}_bn_variance.npy\"):\n",
    "            f.write(\"// bn variance\\n\")\n",
    "            f.write(f\"s16 layer{layer}_bn_variance[] = {{\\n\")\n",
    "            for hex_val in variance_hex:  \n",
    "                f.write(f\"{hex_val},\\n\")  \n",
    "            f.write(\"};\\n\") \n",
    "        \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing BatchNorm parameters: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
